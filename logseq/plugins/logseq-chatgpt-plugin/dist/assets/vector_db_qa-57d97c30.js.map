{"version":3,"file":"vector_db_qa-57d97c30.js","sources":["../../node_modules/langchain/dist/prompts/selectors/conditional.js","../../node_modules/langchain/dist/chains/question_answering/stuff_prompts.js","../../node_modules/langchain/dist/chains/question_answering/load.js","../../node_modules/langchain/dist/chains/vector_db_qa.js"],"sourcesContent":["export class BasePromptSelector {\n    async getPromptAsync(llm, options) {\n        const prompt = this.getPrompt(llm);\n        return prompt.partial(options?.partialVariables ?? {});\n    }\n}\nexport class ConditionalPromptSelector extends BasePromptSelector {\n    constructor(default_prompt, conditionals = []) {\n        super();\n        Object.defineProperty(this, \"defaultPrompt\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"conditionals\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.defaultPrompt = default_prompt;\n        this.conditionals = conditionals;\n    }\n    getPrompt(llm) {\n        for (const [condition, prompt] of this.conditionals) {\n            if (condition(llm)) {\n                return prompt;\n            }\n        }\n        return this.defaultPrompt;\n    }\n}\nexport function isLLM(llm) {\n    return llm._modelType() === \"base_llm\";\n}\nexport function isChatModel(llm) {\n    return llm._modelType() === \"base_chat_model\";\n}\n","/* eslint-disable spaced-comment */\nimport { PromptTemplate } from \"../../prompts/prompt.js\";\nimport { ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, } from \"../../prompts/chat.js\";\nimport { ConditionalPromptSelector, isChatModel, } from \"../../prompts/selectors/conditional.js\";\nexport const DEFAULT_QA_PROMPT = /*#__PURE__*/ new PromptTemplate({\n    template: \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\",\n    inputVariables: [\"context\", \"question\"],\n});\nconst system_template = `Use the following pieces of context to answer the users question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}`;\nconst messages = [\n    /*#__PURE__*/ SystemMessagePromptTemplate.fromTemplate(system_template),\n    /*#__PURE__*/ HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nconst CHAT_PROMPT = \n/*#__PURE__*/ ChatPromptTemplate.fromPromptMessages(messages);\nexport const QA_PROMPT_SELECTOR = /*#__PURE__*/ new ConditionalPromptSelector(DEFAULT_QA_PROMPT, [[isChatModel, CHAT_PROMPT]]);\n","import { LLMChain } from \"../llm_chain.js\";\nimport { StuffDocumentsChain, MapReduceDocumentsChain, RefineDocumentsChain, } from \"../combine_docs_chain.js\";\nimport { QA_PROMPT_SELECTOR } from \"./stuff_prompts.js\";\nimport { COMBINE_PROMPT_SELECTOR, COMBINE_QA_PROMPT_SELECTOR, } from \"./map_reduce_prompts.js\";\nimport { QUESTION_PROMPT_SELECTOR, REFINE_PROMPT_SELECTOR, } from \"./refine_prompts.js\";\nexport const loadQAChain = (llm, params = { type: \"stuff\" }) => {\n    const { type } = params;\n    if (type === \"stuff\") {\n        return loadQAStuffChain(llm, params);\n    }\n    if (type === \"map_reduce\") {\n        return loadQAMapReduceChain(llm, params);\n    }\n    if (type === \"refine\") {\n        return loadQARefineChain(llm, params);\n    }\n    throw new Error(`Invalid _type: ${type}`);\n};\nexport function loadQAStuffChain(llm, params = {}) {\n    const { prompt = QA_PROMPT_SELECTOR.getPrompt(llm), verbose } = params;\n    const llmChain = new LLMChain({ prompt, llm, verbose });\n    const chain = new StuffDocumentsChain({ llmChain, verbose });\n    return chain;\n}\nexport function loadQAMapReduceChain(llm, params = {}) {\n    const { combineMapPrompt = COMBINE_QA_PROMPT_SELECTOR.getPrompt(llm), combinePrompt = COMBINE_PROMPT_SELECTOR.getPrompt(llm), verbose, returnIntermediateSteps, } = params;\n    const llmChain = new LLMChain({ prompt: combineMapPrompt, llm, verbose });\n    const combineLLMChain = new LLMChain({ prompt: combinePrompt, llm, verbose });\n    const combineDocumentChain = new StuffDocumentsChain({\n        llmChain: combineLLMChain,\n        documentVariableName: \"summaries\",\n        verbose,\n    });\n    const chain = new MapReduceDocumentsChain({\n        llmChain,\n        combineDocumentChain,\n        returnIntermediateSteps,\n        verbose,\n    });\n    return chain;\n}\nexport function loadQARefineChain(llm, params = {}) {\n    const { questionPrompt = QUESTION_PROMPT_SELECTOR.getPrompt(llm), refinePrompt = REFINE_PROMPT_SELECTOR.getPrompt(llm), verbose, } = params;\n    const llmChain = new LLMChain({ prompt: questionPrompt, llm, verbose });\n    const refineLLMChain = new LLMChain({ prompt: refinePrompt, llm, verbose });\n    const chain = new RefineDocumentsChain({\n        llmChain,\n        refineLLMChain,\n        verbose,\n    });\n    return chain;\n}\n","import { BaseChain } from \"./base.js\";\nimport { loadQAStuffChain } from \"./question_answering/load.js\";\nexport class VectorDBQAChain extends BaseChain {\n    get inputKeys() {\n        return [this.inputKey];\n    }\n    get outputKeys() {\n        return this.combineDocumentsChain.outputKeys.concat(this.returnSourceDocuments ? [\"sourceDocuments\"] : []);\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"k\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 4\n        });\n        Object.defineProperty(this, \"inputKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"query\"\n        });\n        Object.defineProperty(this, \"vectorstore\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"combineDocumentsChain\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"returnSourceDocuments\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        this.vectorstore = fields.vectorstore;\n        this.combineDocumentsChain = fields.combineDocumentsChain;\n        this.inputKey = fields.inputKey ?? this.inputKey;\n        this.k = fields.k ?? this.k;\n        this.returnSourceDocuments =\n            fields.returnSourceDocuments ?? this.returnSourceDocuments;\n    }\n    /** @ignore */\n    async _call(values, runManager) {\n        if (!(this.inputKey in values)) {\n            throw new Error(`Question key ${this.inputKey} not found.`);\n        }\n        const question = values[this.inputKey];\n        const docs = await this.vectorstore.similaritySearch(question, this.k, values.filter);\n        const inputs = { question, input_documents: docs };\n        const result = await this.combineDocumentsChain.call(inputs, runManager?.getChild(\"combine_documents\"));\n        if (this.returnSourceDocuments) {\n            return {\n                ...result,\n                sourceDocuments: docs,\n            };\n        }\n        return result;\n    }\n    _chainType() {\n        return \"vector_db_qa\";\n    }\n    static async deserialize(data, values) {\n        if (!(\"vectorstore\" in values)) {\n            throw new Error(`Need to pass in a vectorstore to deserialize VectorDBQAChain`);\n        }\n        const { vectorstore } = values;\n        if (!data.combine_documents_chain) {\n            throw new Error(`VectorDBQAChain must have combine_documents_chain in serialized data`);\n        }\n        return new VectorDBQAChain({\n            combineDocumentsChain: await BaseChain.deserialize(data.combine_documents_chain),\n            k: data.k,\n            vectorstore,\n        });\n    }\n    serialize() {\n        return {\n            _type: this._chainType(),\n            combine_documents_chain: this.combineDocumentsChain.serialize(),\n            k: this.k,\n        };\n    }\n    static fromLLM(llm, vectorstore, options) {\n        const qaChain = loadQAStuffChain(llm);\n        return new this({\n            vectorstore,\n            combineDocumentsChain: qaChain,\n            ...options,\n        });\n    }\n}\n"],"names":["BasePromptSelector","llm","options","ConditionalPromptSelector","default_prompt","conditionals","condition","prompt","isChatModel","DEFAULT_QA_PROMPT","PromptTemplate","system_template","messages","SystemMessagePromptTemplate","HumanMessagePromptTemplate","CHAT_PROMPT","ChatPromptTemplate","QA_PROMPT_SELECTOR","loadQAStuffChain","params","verbose","llmChain","LLMChain","StuffDocumentsChain","VectorDBQAChain","BaseChain","fields","values","runManager","question","docs","inputs","result","data","vectorstore","qaChain"],"mappings":"kJAAO,MAAMA,CAAmB,CAC5B,MAAM,eAAeC,EAAKC,EAAS,CAE/B,OADe,KAAK,UAAUD,CAAG,EACnB,SAAQC,GAAA,YAAAA,EAAS,mBAAoB,CAAE,CAAA,CACxD,CACL,CACO,MAAMC,UAAkCH,CAAmB,CAC9D,YAAYI,EAAgBC,EAAe,GAAI,CAC3C,QACA,OAAO,eAAe,KAAM,gBAAiB,CACzC,WAAY,GACZ,aAAc,GACd,SAAU,GACV,MAAO,MACnB,CAAS,EACD,OAAO,eAAe,KAAM,eAAgB,CACxC,WAAY,GACZ,aAAc,GACd,SAAU,GACV,MAAO,MACnB,CAAS,EACD,KAAK,cAAgBD,EACrB,KAAK,aAAeC,CACvB,CACD,UAAUJ,EAAK,CACX,SAAW,CAACK,EAAWC,CAAM,IAAK,KAAK,aACnC,GAAID,EAAUL,CAAG,EACb,OAAOM,EAGf,OAAO,KAAK,aACf,CACL,CAIO,SAASC,EAAYP,EAAK,CAC7B,OAAOA,EAAI,WAAY,IAAK,iBAChC,CClCO,MAAMQ,EAAkC,IAAIC,EAAe,CAC9D,SAAU;AAAA;AAAA;AAAA;AAAA;AAAA,iBACV,eAAgB,CAAC,UAAW,UAAU,CAC1C,CAAC,EACKC,EAAkB;AAAA;AAAA;AAAA,WAIlBC,EAAW,CACCC,EAA4B,aAAaF,CAAe,EACxDG,EAA2B,aAAa,YAAY,CACtE,EACMC,EACQC,EAAmB,mBAAmBJ,CAAQ,EAC/CK,EAAmC,IAAId,EAA0BM,EAAmB,CAAC,CAACD,EAAaO,CAAW,CAAC,CAAC,ECAtH,SAASG,EAAiBjB,EAAKkB,EAAS,GAAI,CAC/C,KAAM,CAAE,OAAAZ,EAASU,EAAmB,UAAUhB,CAAG,EAAG,QAAAmB,CAAS,EAAGD,EAC1DE,EAAW,IAAIC,EAAS,CAAE,OAAAf,EAAQ,IAAAN,EAAK,QAAAmB,CAAO,CAAE,EAEtD,OADc,IAAIG,EAAoB,CAAE,SAAAF,EAAU,QAAAD,CAAS,CAAA,CAE/D,CCrBO,MAAMI,UAAwBC,CAAU,CAC3C,IAAI,WAAY,CACZ,MAAO,CAAC,KAAK,QAAQ,CACxB,CACD,IAAI,YAAa,CACb,OAAO,KAAK,sBAAsB,WAAW,OAAO,KAAK,sBAAwB,CAAC,iBAAiB,EAAI,CAAA,CAAE,CAC5G,CACD,YAAYC,EAAQ,CAChB,MAAMA,CAAM,EACZ,OAAO,eAAe,KAAM,IAAK,CAC7B,WAAY,GACZ,aAAc,GACd,SAAU,GACV,MAAO,CACnB,CAAS,EACD,OAAO,eAAe,KAAM,WAAY,CACpC,WAAY,GACZ,aAAc,GACd,SAAU,GACV,MAAO,OACnB,CAAS,EACD,OAAO,eAAe,KAAM,cAAe,CACvC,WAAY,GACZ,aAAc,GACd,SAAU,GACV,MAAO,MACnB,CAAS,EACD,OAAO,eAAe,KAAM,wBAAyB,CACjD,WAAY,GACZ,aAAc,GACd,SAAU,GACV,MAAO,MACnB,CAAS,EACD,OAAO,eAAe,KAAM,wBAAyB,CACjD,WAAY,GACZ,aAAc,GACd,SAAU,GACV,MAAO,EACnB,CAAS,EACD,KAAK,YAAcA,EAAO,YAC1B,KAAK,sBAAwBA,EAAO,sBACpC,KAAK,SAAWA,EAAO,UAAY,KAAK,SACxC,KAAK,EAAIA,EAAO,GAAK,KAAK,EAC1B,KAAK,sBACDA,EAAO,uBAAyB,KAAK,qBAC5C,CAED,MAAM,MAAMC,EAAQC,EAAY,CAC5B,GAAI,EAAE,KAAK,YAAYD,GACnB,MAAM,IAAI,MAAM,gBAAgB,KAAK,qBAAqB,EAE9D,MAAME,EAAWF,EAAO,KAAK,QAAQ,EAC/BG,EAAO,MAAM,KAAK,YAAY,iBAAiBD,EAAU,KAAK,EAAGF,EAAO,MAAM,EAC9EI,EAAS,CAAE,SAAAF,EAAU,gBAAiBC,CAAI,EAC1CE,EAAS,MAAM,KAAK,sBAAsB,KAAKD,EAAQH,GAAA,YAAAA,EAAY,SAAS,oBAAoB,EACtG,OAAI,KAAK,sBACE,CACH,GAAGI,EACH,gBAAiBF,CACjC,EAEeE,CACV,CACD,YAAa,CACT,MAAO,cACV,CACD,aAAa,YAAYC,EAAMN,EAAQ,CACnC,GAAI,EAAE,gBAAiBA,GACnB,MAAM,IAAI,MAAM,8DAA8D,EAElF,KAAM,CAAE,YAAAO,CAAa,EAAGP,EACxB,GAAI,CAACM,EAAK,wBACN,MAAM,IAAI,MAAM,sEAAsE,EAE1F,OAAO,IAAIT,EAAgB,CACvB,sBAAuB,MAAMC,EAAU,YAAYQ,EAAK,uBAAuB,EAC/E,EAAGA,EAAK,EACR,YAAAC,CACZ,CAAS,CACJ,CACD,WAAY,CACR,MAAO,CACH,MAAO,KAAK,WAAY,EACxB,wBAAyB,KAAK,sBAAsB,UAAW,EAC/D,EAAG,KAAK,CACpB,CACK,CACD,OAAO,QAAQjC,EAAKiC,EAAahC,EAAS,CACtC,MAAMiC,EAAUjB,EAAiBjB,CAAG,EACpC,OAAO,IAAI,KAAK,CACZ,YAAAiC,EACA,sBAAuBC,EACvB,GAAGjC,CACf,CAAS,CACJ,CACL","x_google_ignoreList":[0,1,2,3]}